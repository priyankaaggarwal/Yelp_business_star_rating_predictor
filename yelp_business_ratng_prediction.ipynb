{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML: Predicting Star Ratings\n",
    "Objective : To predict a new venue's popularity from information available when the venue opens.  \n",
    "Dataset : Venue popularities dataset provided by Yelp.  The data set contains meta data about the venue (where it is located, the type of food served, etc.).  It also contains a star rating. Note that the venues are not limited to restaurants. \n",
    "\n",
    "Metric: Root mean squared error of the number of stars predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and parse the incoming data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data are a series of JSON objects, in a Gzipped file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import gzip\n",
    "\n",
    "with gzip.open('yelp_train_academic_dataset_business.json.gz') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records:  37938\n",
      "Features: \n",
      " dict_keys(['business_id', 'full_address', 'hours', 'open', 'categories', 'city', 'review_count', 'name', 'neighborhoods', 'longitude', 'state', 'stars', 'latitude', 'attributes', 'type'])\n",
      "Datatype for features: \n",
      "<class 'str'> <class 'str'> <class 'str'> <class 'str'> <class 'str'> <class 'str'> <class 'str'> <class 'str'> <class 'str'> <class 'str'> <class 'str'> <class 'str'> <class 'str'> <class 'str'> <class 'str'> "
     ]
    }
   ],
   "source": [
    "print(\"Total records: \", len(data))\n",
    "print(\"Features: \\n\",data[0].keys())\n",
    "print(\"Datatype for features: \")\n",
    "for d in data[0]:\n",
    "    print(type(d),end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "star_ratings = [row['stars'] for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the dataset into Train and Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data , star_ratings, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28453, 9485)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the \"model\" questions asks you to create a function that models the number of stars venues will receive.  It will be passed a list of dictionaries.  Each of these will have the same format as the JSON objects you've just read in.  Some of the keys (like the stars!) will have been removed.  This function should return a list of numbers of the same length, indicating the predicted star ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using city_avg as the feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The venues belong to different cities.  It can be assumed that the ratings in some cities are probably higher than others.  We wish to build an estimator to make a prediction based on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will work out the average rating for each city and create a list of tuples (city name, star rating), one for each city in the dataset. For that, calculate the sum of the star ratings and the number of venues for each city.  At the end, we can just divide the stars by the count to get the average.\n",
    "\n",
    "We will use collections module's `defaultdict` class that provides default values for keys that haven't been used. This will help to test whether a key exists in the dictionary before adding to the running tally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "star_sum = defaultdict(int)\n",
    "count = defaultdict(int)\n",
    "\n",
    "for row, stars in zip(data, star_ratings):\n",
    "    city = row['city']\n",
    "    star_sum[city]+=stars\n",
    "    count[city]+=1\n",
    "    \n",
    "avg_stars = dict()\n",
    "for city in star_sum:\n",
    "    # calculate average star rating and store in avg_stars\n",
    "    avg_stars[city] = star_sum[city]/count[city]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cities:  167\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "dict(islice(avg_stars.items(),10))\n",
    "print (\"Number of cities: \", len(avg_stars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build a custom estimator that will make a prediction based solely on the city of a venue.  \n",
    "\n",
    "This custom estimator will have a `.fit()` method.  It will receive `data` as its argument `X` and `star_ratings` as `y`, and should repeat the calculation of the previous problem there.  \n",
    "\n",
    "Then the `.predict()` method can look up the average rating for the city of each record it receives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import base\n",
    "\n",
    "class CityEstimator(base.BaseEstimator, base.RegressorMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.avg_stars = dict()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Store the average rating per city in self.avg_stars\n",
    "        star_total = defaultdict(int)\n",
    "        count = defaultdict(int)\n",
    "        for row, star_rating in zip(X,y):\n",
    "            city = row['city']\n",
    "            star_total[city]+=star_rating\n",
    "            count[city]+=1\n",
    "            self.avg_stars[city] = star_total[city]/count[city]\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # return city average if city exists in training set, otherwise return mean of all cities\n",
    "        return [self.avg_stars.get(row['city'])  if self.avg_stars.get(row['city']) else np.array([self.avg_stars[city] for city in self.avg_stars]).mean() for row in X ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an instance of our estimator and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CityEstimator()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_est = CityEstimator()\n",
    "# city_est.fit(data, star_ratings)\n",
    "city_est.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.800321702157251"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, city_est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the model is performing very poorly. Let's see how we can improve it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using business latitude and longitude as a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City-based model is not sufficiently fine-grained. For example, we know that some neighborhoods are trendier than others.  To solve that, let's use the latitude and longitude of a venue as features to understand neighborhood dynamics.\n",
    "\n",
    "Instead of writing a custom estimator, we'll use one of the built-in estimators in Scikit Learn.  Since these estimators won't know what to do with a list of dictionaries, we'll build a `ColumnSelectTransformer` that will return an array containing selected keys of our feature matrix.  \n",
    "\n",
    "We will make this transformer generic such that it works on an arbitrary list of columns making it very useful in the future as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnSelectTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col_names):\n",
    "        self.col_names = col_names  # We will need these in transform()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Return an array with the same number of rows as X and one\n",
    "        # column for each in self.col_names\n",
    "        X_trans = []\n",
    "        for row in X:\n",
    "            X_trans.append([row[col] for col in self.col_names])\n",
    "        return X_trans\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on a single row, just as a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "cst = ColumnSelectTransformer(['latitude', 'longitude'])\n",
    "assert (cst.fit_transform(X_train[:1])\n",
    "        == [[X_train[0]['latitude'], X_train[0]['longitude']]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's feed the output of the transformer in to a `sklearn.neighbors.KNeighborsRegressor`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                    metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                    weights='uniform')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "data_transform = cst.fit_transform(X_train)\n",
    "knn = KNeighborsRegressor(n_neighbors=5)\n",
    "knn.fit(data_transform, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we'll test it with the first 5 rows.  To truly judge the performance, we'd need to make a test/train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4. , 3.5, 3.9, 3.7, 3.8]), [3.5, 3.0, 1.5, 3.5, 4.0])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = X_train[:5]\n",
    "test_data_transform = cst.transform(test_data)\n",
    "knn.predict(test_data_transform), y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing this by hand, let's make a pipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('preprocess',cst), # ColumnSelectTransformer\n",
    "                 ('model',knn)       # KNeighborsRegressor\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should work the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4. , 3.5, 3.9, 3.7, 3.8])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "pipe.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used `n_neighbors = 5` as hyperparameter to `KNeighborsRegressor`, which tells it how many nearest neighbors to average together when making a prediction.  \n",
    "We do now know if 5 is the optimum value. So let's determine a better value of this hyperparameter.   There are several ways to do this:\n",
    "\n",
    "1. Use [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) to split your data in to a training set and a test set.  Score the performance on the test set.  After finding the best hyperparameter, retrain the model on the full data at that hyperparameter value.\n",
    "\n",
    "2. Use [`cross_val_score`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) to return cross-validation scores on your data for various values of the hyperparameter.  Choose the best one, and retrain the model on the full data.\n",
    "\n",
    "3. Use [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) to do the splitting, training, and grading automatically.  \n",
    "\n",
    "<b> We will be using `GridSearchCV` </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "cst = ColumnSelectTransformer(['latitude', 'longitude'])\n",
    "data_transform = cst.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 7 folds for each of 30 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:   55.6s\n",
      "[Parallel(n_jobs=2)]: Done 210 out of 210 | elapsed:   59.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocess_data',\n",
       "                 ColumnSelectTransformer(col_names=['latitude', 'longitude'])),\n",
       "                ('grid_estimator',\n",
       "                 GridSearchCV(cv=7, error_score='raise-deprecating',\n",
       "                              estimator=KNeighborsRegressor(algorithm='auto',\n",
       "                                                            leaf_size=30,\n",
       "                                                            metric='minkowski',\n",
       "                                                            metric_params=None,\n",
       "                                                            n_jobs=None,\n",
       "                                                            n_neighbors=5, p=2,\n",
       "                                                            weights='uniform'),\n",
       "                              iid='warn', n_jobs=2,\n",
       "                              param_grid={'n_neighbors': [20, 21, 22, 23, 24,\n",
       "                                                          25, 26, 27, 28, 29,\n",
       "                                                          30, 31, 32, 33, 34,\n",
       "                                                          35, 36, 37, 38, 39,\n",
       "                                                          40, 41, 42, 43, 44,\n",
       "                                                          45, 46, 47, 48, 49]},\n",
       "                              pre_dispatch='2*n_jobs', refit=True,\n",
       "                              return_train_score=False, scoring=None,\n",
       "                              verbose=1))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using GridSearchCV in Pipeline to compute n_neighbors value\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cst = ColumnSelectTransformer(['latitude', 'longitude'])\n",
    "param_grid = {'n_neighbors':list(range(20,50))}\n",
    "gs = GridSearchCV(KNeighborsRegressor(), param_grid, cv=7, n_jobs=2, verbose=1)\n",
    "lat_long_est = Pipeline([('preprocess_data',ColumnSelectTransformer(['latitude', 'longitude'])),\n",
    "                         ('grid_estimator',gs)])\n",
    "lat_long_est.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 48}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lat_long_est['grid_estimator'].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7854394658232297"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = lat_long_est.predict(X_test)\n",
    "mean_squared_error(y_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did reduce error a little. But still there is just rather little signal available for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using business category as a feature:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While location is important, we could also try seeing how predictive the\n",
    "venue's category is.  We'll build an estimator that considers only the categories.\n",
    "\n",
    "The categories come as a list of strings, but the built-in estimators all need numeric input.  We will use  **one-hot encoding**, also known as dummy variables to deal with categorical features. \n",
    "\n",
    "The `ColumnSelectTransformer` we built earlier can be used to extract the categories column as a list of strings. \n",
    "\n",
    "This list of strings will be converted to a list of dictionaries using custom class `DictEncoder`\n",
    "\n",
    "We will use Scikit Learn [`DictVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer), which takes in a list of dictionaries.  It creates a column in the output matrix for each key in the dictionary and fills it with the value associated with it.  Missing keys are filled with zeros.  Therefore, we need only build a transformer that takes a list of strings to a dictionary with keys given by those strings and values of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Doctors', 'Health & Medical']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'business_id': 'vcNAWiLM4dR7D2nwwJ7nCA',\n",
       " 'full_address': '4840 E Indian School Rd\\nSte 101\\nPhoenix, AZ 85018',\n",
       " 'hours': {'Tuesday': {'close': '17:00', 'open': '08:00'},\n",
       "  'Friday': {'close': '17:00', 'open': '08:00'},\n",
       "  'Monday': {'close': '17:00', 'open': '08:00'},\n",
       "  'Wednesday': {'close': '17:00', 'open': '08:00'},\n",
       "  'Thursday': {'close': '17:00', 'open': '08:00'}},\n",
       " 'open': True,\n",
       " 'categories': ['Doctors', 'Health & Medical'],\n",
       " 'city': 'Phoenix',\n",
       " 'review_count': 7,\n",
       " 'name': 'Eric Goldberg, MD',\n",
       " 'neighborhoods': [],\n",
       " 'longitude': -111.983758,\n",
       " 'state': 'AZ',\n",
       " 'stars': 3.5,\n",
       " 'latitude': 33.499313,\n",
       " 'attributes': {'By Appointment Only': True},\n",
       " 'type': 'business'}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data[0]['categories'])\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictEncoder(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # X will come in as a list of lists of lists.  \n",
    "        # Return a list of dictionaries corresponding to those inner lists.\n",
    "        X_list=[]\n",
    "        for row in X:\n",
    "            categories_dict = {cat:1 for cats in row for cat in cats}\n",
    "            X_list.append(categories_dict)\n",
    "        return X_list\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DictEncoder\n",
    "assert (DictEncoder().fit_transform([[['a']], [['b', 'c']]])\n",
    "        == [{'a': 1}, {'b': 1, 'c': 1}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will set up a pipeline with our `ColumnSelectTransformer`, our `DictEncoder`, the `DictVectorizer`, and a regularized linear model, like `Ridge`, as the estimator.  \n",
    "Also, some categories (e.g. Restaurants) are not very specific.  Others (Japanese sushi) are much more so.  One way to deal with this is using term-frequency-inverse-document-frequency (TF-IDF) between the `DictVectorizer` and the linear model.\n",
    "\n",
    "This model will have a large number of features, one for each category, so there is a significant danger of overfitting.  We will use cross validation to choose the best regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('col_transform',\n",
       "                 ColumnSelectTransformer(col_names=['categories'])),\n",
       "                ('dict_encode', DictEncoder()),\n",
       "                ('dict_vectorize',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=False)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('ridge',\n",
       "                 Ridge(alpha=0.2, copy_X=True, fit_intercept=True,\n",
       "                       max_iter=None, normalize=False, random_state=None,\n",
       "                       solver='auto', tol=0.001))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "cst_categories = ColumnSelectTransformer(['categories'])\n",
    "dict_encode = DictEncoder()\n",
    "dict_vectorizer = DictVectorizer(sparse=False)\n",
    "tfidf_transf = TfidfTransformer()\n",
    "ridge = Ridge(alpha=0.2)\n",
    "\n",
    "category_est = Pipeline(steps=[('col_transform', cst_categories),\n",
    "                               ('dict_encode',dict_encode),\n",
    "                               ('dict_vectorize', dict_vectorizer),\n",
    "                               ('tfidf', tfidf_transf),\n",
    "                               ('ridge',ridge)])\n",
    "category_est.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = category_est.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6590591211272805"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With business categories as feature, we were able to further reduce the error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model based on business attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is even more information in the attributes for each venue.  Let's build an estimator based on these.\n",
    "\n",
    "Venues attributes may be nested:\n",
    "```\n",
    "{\n",
    "  'Attire': 'casual',\n",
    "  'Accepts Credit Cards': True,\n",
    "  'Ambiance': {'casual': False, 'classy': False}\n",
    "}\n",
    "```\n",
    "We wish to encode them with one-hot encoding.  The `DictVectorizer` can do this, but only once we've flattened the dictionary to a single level, like so:\n",
    "```\n",
    "{\n",
    "  'Attire_casual' : 1,\n",
    "  'Accepts Credit Cards': 1,\n",
    "  'Ambiance_casual': 0,\n",
    "  'Ambiance_classy': 0\n",
    "}\n",
    "```\n",
    "\n",
    "We'll build a custom transformer that flattens the attributes dictionary and place this in a pipeline with a `DictVectorizer` and a regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NestedDictionaryTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "       \n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_trans=[]\n",
    "        for row in X:\n",
    "            new_dict = dict()\n",
    "            for feature in row:\n",
    "                for attr,attr_val in feature.items():\n",
    "                    if (isinstance(attr_val,bool)):\n",
    "                        k=attr\n",
    "                        v=int(attr_val)\n",
    "                        new_dict[k]=v\n",
    "                    elif (isinstance(attr_val,str)):\n",
    "                        k = attr+'_'+attr_val\n",
    "                        v = 1\n",
    "                        new_dict[k]=v\n",
    "                    elif (isinstance(attr_val,dict)):\n",
    "                        for key,val in attr_val.items():\n",
    "                            k = attr+\"_\"+key\n",
    "                            if (isinstance(val,bool)):\n",
    "                                v=int(val)\n",
    "                            elif (isinstance(attr_val,str)):\n",
    "                                v = 1\n",
    "                            new_dict[k]=v\n",
    "#                 row[j]=new_dict\n",
    "            X_trans.append(new_dict)\n",
    "        return X_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndt = NestedDictionaryTransformer()\n",
    "ndt_t = DictVectorizer(sparse=False).fit_transform( \\\n",
    "                                                   ndt.fit_transform( \\\n",
    "                                                                     ColumnSelectTransformer(['attributes']).transform(X_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndt_t[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be difficult to find a single regressor that does well enough.  We can address this by using a linear model to fit the linear part of data, and use a non-linear model to fit the residual that the linear model can't fit.  That is, build a residual estimator that takes as an argument two other estimators.  It should use the first to fit the raw data and the second to fit the residuals of the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "class LinearAndNonlinearEstimator(base.BaseEstimator, base.RegressorMixin):\n",
    "    \n",
    "    def __init__(self,linear, non_linear):\n",
    "        self.avg_stars = []\n",
    "        self.non_linear = non_linear\n",
    "        self.linear = linear\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.linear.fit(X,y)\n",
    "        y_hat = self.linear.predict(X)\n",
    "        residual = y-y_hat\n",
    "        self.non_linear.fit(X, residual)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.linear.predict(X)+self.non_linear.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "cst_attribute = ColumnSelectTransformer(['attributes'])\n",
    "nested_dic_transformer = NestedDictionaryTransformer()\n",
    "dict_vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "pipe_transformed = Pipeline(steps=[('cst_attributes', cst_attribute),\n",
    "                                  ('nested_dic_transformer',nested_dic_transformer),\n",
    "                                  ('dict_vectorizer',dict_vectorizer)])\n",
    "X_t = pipe_transformed.fit_transform(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use GridSearchCV to compute best parameters for linear regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 39 candidates, totalling 195 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done 123 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=2)]: Done 195 out of 195 | elapsed:    4.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True,\n",
       "                             max_iter=None, normalize=False, random_state=None,\n",
       "                             solver='auto', tol=0.001),\n",
       "             iid='warn', n_jobs=2,\n",
       "             param_grid={'alpha': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
       "                                   14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24,\n",
       "                                   25, 26, 27, 28, 29, 30, ...]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forest = RandomForestRegressor(n_estimators=80, min_samples_split=15,random_state=100)\n",
    "gs_ridge = GridSearchCV(Ridge(),param_grid={'alpha':list(range(1,40))}, cv=5, verbose=1, n_jobs=2)\n",
    "\n",
    "gs_ridge.fit(X_t,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 15}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_ridge.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ridge = gs_ridge.predict(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_residual = y_train - y_ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use GridSearchCV to compute best parameters for non-linear regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 44 candidates, totalling 220 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=2)]: Done 220 out of 220 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': 55, 'n_estimators': 100}"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_params = {'n_estimators':[10,50,75,100],\n",
    "                 'min_samples_split':list(range(5,60,5))}\n",
    "gs_forest = GridSearchCV(RandomForestRegressor(),param_grid=forest_params, cv=5, verbose=1, n_jobs=2)\n",
    "gs_forest.fit(X_t,y_residual)\n",
    "gs_forest.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the hyperparameters from GridSearch, we will build a pipeline for data transformation and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cst_attributes',\n",
       "                 ColumnSelectTransformer(col_names=['attributes'])),\n",
       "                ('nested_dic_transformer', NestedDictionaryTransformer()),\n",
       "                ('dict_vectorizer',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=False)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('lnl',\n",
       "                 L...\n",
       "                                                          tol=0.001),\n",
       "                                             non_linear=RandomForestRegressor(bootstrap=True,\n",
       "                                                                              criterion='mse',\n",
       "                                                                              max_depth=None,\n",
       "                                                                              max_features='auto',\n",
       "                                                                              max_leaf_nodes=None,\n",
       "                                                                              min_impurity_decrease=0.0,\n",
       "                                                                              min_impurity_split=None,\n",
       "                                                                              min_samples_leaf=1,\n",
       "                                                                              min_samples_split=55,\n",
       "                                                                              min_weight_fraction_leaf=0.0,\n",
       "                                                                              n_estimators=100,\n",
       "                                                                              n_jobs=None,\n",
       "                                                                              oob_score=False,\n",
       "                                                                              random_state=0,\n",
       "                                                                              verbose=0,\n",
       "                                                                              warm_start=False)))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "cst_attribute = ColumnSelectTransformer(['attributes'])\n",
    "nested_dic_transformer = NestedDictionaryTransformer()\n",
    "dict_vectorizer = DictVectorizer(sparse=False)\n",
    "\n",
    "forest = RandomForestRegressor(n_estimators=100, min_samples_split=55,random_state=0)\n",
    "lnl = LinearAndNonlinearEstimator(Ridge(alpha=15),forest)\n",
    "pipe_attributes = Pipeline(steps=[('cst_attributes', cst_attribute),\n",
    "                                  ('nested_dic_transformer',nested_dic_transformer),\n",
    "                                  ('dict_vectorizer',dict_vectorizer),\n",
    "                                  ('tfidf', TfidfTransformer()),\n",
    "                                  ('lnl',lnl)])\n",
    "\n",
    "pipe_attributes.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7340276030103203"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = pipe_attributes.predict(X_test)\n",
    "mean_squared_error(y_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a full model using all the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only built models based on individual features.  Now we will build an ensemble regressor that averages together the estimates of the four previous regressors.\n",
    "\n",
    "In order to use the existing models as input to an estimator, we will have to turn them into transformers.  (A pipeline can contain at most a single estimator.)  \n",
    "- We will build a custom `ModelTransformer` class that takes an estimator as an argument.  When `fit()` is called, the estimator should be fit.  When `transform()` is called, the estimator's `predict()` method should be called, and its results returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstimatorTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, estimator):\n",
    "        self.estimator=estimator\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Fit the stored estimator.\n",
    "        self.estimator.fit(X,y)\n",
    "        return self # for chaining, its important to return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Use predict on the stored estimator as a \"transformation\".\n",
    "        return np.array(self.estimator.predict(X)).reshape(-1,1) # transform() should be a 2-D array with a single column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should work as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_trans = EstimatorTransformer(city_est)\n",
    "city_trans.fit(X_train, y_train)\n",
    "\n",
    "assert ([r[0] for r in city_trans.transform(X_train[:5])]\n",
    "        == city_est.predict(X_train[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of `ModelTransformer` for each of the previous four models. Combine these together in a single feature matrix with a\n",
    "[`FeatureUnion`](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "city_est = CityEstimator()\n",
    "\n",
    "lat_long_est = Pipeline([('preprocess_data',ColumnSelectTransformer(['latitude', 'longitude'])),\n",
    "                         ('optimized_model',KNeighborsRegressor(n_neighbors=30))])\n",
    "\n",
    "category_est = Pipeline(steps=[('col_transform',  ColumnSelectTransformer(['categories'])),\n",
    "                               ('dict_encode',DictEncoder()),\n",
    "                               ('dict_vectorize', DictVectorizer(sparse=False)),\n",
    "                               ('tfidf', TfidfTransformer()),\n",
    "                               ('ridge',Ridge(alpha=0.2))])\n",
    "\n",
    "\n",
    "pipe_attributes = Pipeline(steps=[('cst_attributes', ColumnSelectTransformer(['attributes'])),\n",
    "                                  ('nested_dic_transformer',NestedDictionaryTransformer()),\n",
    "                                  ('dict_vectorizer',DictVectorizer(sparse=False)),\n",
    "                                  ('lnl',LinearAndNonlinearEstimator(Ridge(alpha=16),RandomForestRegressor(n_estimators=100, min_samples_split=35,random_state=100)))])\n",
    "union = FeatureUnion([\n",
    "        # FeatureUnions use the same syntax as Pipelines\n",
    "    ('city_est',EstimatorTransformer(city_est)),\n",
    "    ('lat_long_est',EstimatorTransformer(lat_long_est)),\n",
    "    ('category_est', EstimatorTransformer(category_est)),\n",
    "    ('pipe_attributes', EstimatorTransformer(pipe_attributes))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should return a feature matrix with four columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans_data:  [[3.64940264 3.95       3.79522399 3.95081885]\n",
      " [3.64940264 3.61666667 3.44200052 3.44874792]\n",
      " [3.64940264 3.93333333 3.20695889 3.95081885]\n",
      " [3.65549005 3.43333333 3.27679335 3.38878487]\n",
      " [3.66666667 3.76666667 3.21105972 4.10628361]\n",
      " [3.67213598 4.25       3.96991437 3.67892706]\n",
      " [3.65549005 3.43333333 3.41878343 3.55544491]\n",
      " [3.70606061 4.03333333 4.27592511 3.95081885]\n",
      " [3.816875   3.55       4.06436874 3.67892706]\n",
      " [3.67213598 3.81666667 3.79925182 3.77252277]]\n"
     ]
    }
   ],
   "source": [
    "union.fit(X_train,y_train)\n",
    "trans_data = union.transform(X_train[:10])\n",
    "assert trans_data.shape == (10, 4)\n",
    "print (\"trans_data: \",trans_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use a pipeline to combine the feature union with a linear regression model to weight the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By combining our models with a linear model, we will be unable to notice any correlation between features.  We don't expect all attributes to have the same effect on all venues.  For example, \"Ambiance: divey\" might be a bad indicator for a restaurant but a good one for a bar.  Nonlinear models can pick up on this interaction.  We'll use a linear model and then a nonlinear model to fit the residuals of the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = union.transform(X_train)\n",
    "lr = LinearRegression()\n",
    "yhat_lr = lr.fit(X_t,y_train).predict(X_t)\n",
    "yhat_res = y_train - yhat_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "             estimator=RandomForestRegressor(bootstrap=True, criterion='mse',\n",
       "                                             max_depth=None,\n",
       "                                             max_features='auto',\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             n_estimators='warn', n_jobs=None,\n",
       "                                             oob_score=False, random_state=None,\n",
       "                                             verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'min_samples_split': [5, 10, 15, 20, 25, 30, 35, 40,\n",
       "                                               45, 50, 55],\n",
       "                         'n_estimators': [10, 50, 80, 100, 120]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lin = GridSearchCV(RandomForestRegressor(), param_grid={'n_estimators': [10,50,80,100,120], 'min_samples_split':list(range(5,60,5))})\n",
    "grid_lin.fit(X_t, yhat_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_split': 55, 'n_estimators': 120}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_lin.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('union',\n",
       "                 FeatureUnion(n_jobs=None,\n",
       "                              transformer_list=[('city_est',\n",
       "                                                 EstimatorTransformer(estimator=CityEstimator())),\n",
       "                                                ('lat_long_est',\n",
       "                                                 EstimatorTransformer(estimator=Pipeline(memory=None,\n",
       "                                                                                         steps=[('preprocess_data',\n",
       "                                                                                                 ColumnSelectTransformer(col_names=['latitude',\n",
       "                                                                                                                                    'longitude'])),\n",
       "                                                                                                ('optimized_model',\n",
       "                                                                                                 KNeighborsRegressor(algorithm='auto...\n",
       "                                                                     normalize=False),\n",
       "                                             non_linear=RandomForestRegressor(bootstrap=True,\n",
       "                                                                              criterion='mse',\n",
       "                                                                              max_depth=None,\n",
       "                                                                              max_features='auto',\n",
       "                                                                              max_leaf_nodes=None,\n",
       "                                                                              min_impurity_decrease=0.0,\n",
       "                                                                              min_impurity_split=None,\n",
       "                                                                              min_samples_leaf=1,\n",
       "                                                                              min_samples_split=35,\n",
       "                                                                              min_weight_fraction_leaf=0.0,\n",
       "                                                                              n_estimators=100,\n",
       "                                                                              n_jobs=None,\n",
       "                                                                              oob_score=False,\n",
       "                                                                              random_state=100,\n",
       "                                                                              verbose=0,\n",
       "                                                                              warm_start=False)))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_est = Pipeline([('union',  union),\n",
    "                     ('lin',LinearAndNonlinearEstimator(LinearRegression(),RandomForestRegressor(n_estimators=100, min_samples_split=35,random_state=100)))])\n",
    "full_est.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6568391283888111"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = full_est.predict(X_test)\n",
    "mean_squared_error(y_test, y_hat) #0.644"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
